{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code M Project\n",
    "\n",
    "Agenda\n",
    "1. Project Overview\n",
    "2. Architecture\n",
    "3. Class hieracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-deab7fc6e015>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mencodings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import requests\n",
    "import encodings\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from pymongo import MongoClient, DESCENDING, ASCENDING\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Configuration classes\n",
    "全局配置类\n",
    "    * configuration.py\n",
    "    * globalvars.py\n",
    "    * globalutilities.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configuration.py\n",
    "#### Database configurations\n",
    "```\n",
    "DB_URI = 'mongodb://localhost' 目标数据库URI\n",
    "DB_NAME = 'CodeManekineko' 目标数据库名称\n",
    "```\n",
    "#### Period execution time\n",
    "```\n",
    "PERIOD_MIN = 5 执行周期间隔（分钟）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" To store the global configuration variables \"\"\"\n",
    "\n",
    "# Database name\n",
    "DB_NAME = 'CodeManekineko'\n",
    "DB_URI='mongodb://localhost'\n",
    "\n",
    "# Period execution sleep time\n",
    "PERIOD_MIN = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utilDB\n",
    "与数据库相关的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnumDbType(Enum):\n",
    "    MongoDB = 1,\n",
    "    DocumentDB = 2\n",
    "\n",
    "class UtilDb:\n",
    "    \"\"\"\n",
    "    Utility class for MongoDB operation\n",
    "    Sample of usage:\n",
    "    util_db = UtilDB('database_name')\n",
    "    print(util_db.is_exist('articleList',{'title':'sample title'}))\n",
    "    \"\"\"\n",
    "    database_type = None\n",
    "    client = None\n",
    "    db = None\n",
    "\n",
    "    def __init__(self, db_name, db_type=EnumDbType.MongoDB, uri=''):\n",
    "        \"\"\" Class initialization\n",
    "        keyword arguments:\n",
    "        db_name: the database name wants to manipulate\n",
    "        db_type: by default EnumDBType.MongoDB\n",
    "        uri: the uri for db connection, leave blank for localhost access\n",
    "        \"\"\"\n",
    "\n",
    "        self.database_type = db_type\n",
    "        self.__db_client_initialization(self.database_type, uri)\n",
    "        self.db = self._get_db(db_name)\n",
    "\n",
    "    def __del__(self):\n",
    "        return\n",
    "\n",
    "    def __db_client_initialization(self, db_type=EnumDbType.MongoDB, uri=''):\n",
    "        \"\"\" Initialize the databse connection\n",
    "        keyword arguments:\n",
    "        db_type: by default is EnumDBType.MongoDB\n",
    "        uri: by default connect to localhost\n",
    "        \"\"\"\n",
    "        target_uri = uri\n",
    "        if target_uri == '':\n",
    "            raise Exception('Configuration item DB_URI is missing in configuration.py.')\n",
    "            target_uri = 'mongodb://localhost'\n",
    "\n",
    "        try:\n",
    "            logging.debug('Request DB connection from %s.' % target_uri)\n",
    "            self.client = MongoClient(target_uri)\n",
    "\n",
    "            # Mongo DB on Azure URI demo\n",
    "            # uri = \"mongodb://code-manekineko:woL44kcE6OD3yUbrFuZ7Rv06wsDGRpblJYU1yWek8GDogbhoubho5mhihCgBDDnKAhluofqykKecFN1hAwUQXg==@code-manekineko.documents.azure.com:10250/?ssl=true&ssl_cert_reqs=CERT_NONE\"\n",
    "        except Exception as e:\n",
    "            # Print error message and halt application(No db connection and nothing can do)\n",
    "            print('###### App halt during initialize the database connection: %s ######' % e)\n",
    "            raise\n",
    "\n",
    "    # Private method\n",
    "    # To specify the operation dbl\n",
    "    def _get_db(self, db_name):\n",
    "        try:\n",
    "            db = self.client[db_name]\n",
    "            return db\n",
    "        except Exception as e:\n",
    "            print('###### App halt as error occurred: %s ######' % e)\n",
    "            raise\n",
    "\n",
    "    # Utility methods\n",
    "    # ===================\n",
    "\n",
    "    # Check if record exist in specific collection.\n",
    "    def is_exist(self, target_collection, filters):\n",
    "        if self.db[target_collection].find(filters).count() == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    # insert document to specific collection\n",
    "    def insert(self, target_collection, document):\n",
    "        self.db[target_collection].insert_one(document)\n",
    "\n",
    "    def find(self,\n",
    "             target_collection,\n",
    "             return_the_first_found=False,\n",
    "             query_filter=None,\n",
    "             sort='',\n",
    "             sort_descending=False,\n",
    "             page_size=0,\n",
    "             page_index=0,\n",
    "             no_cursor_timeout=False):\n",
    "        \"\"\"\n",
    "        Refactored Find operation for MongoDB to replace the find_one, find_all\n",
    "        :param target_collection: the target collection to manipulate.\n",
    "        :param return_the_first_found: set to True to just return the first element found, same as find_one operation.\n",
    "        :param query_filter: dict object, the same usage as MongoDB find query. leave it None to return all.\n",
    "        :param sort: the column needs to sort, by default is blank, which means no sort needed.\n",
    "        :param sort_descending: by default is False\n",
    "        :param page_size: by default no paging, and not applicable when find_all is False\n",
    "        :param page_index: not applicable when page_size=0\n",
    "        :param no_cursor_timeout: Set to True to ignore cursor timeout which usually use during long time data processing.\n",
    "        :return: find_all is True returns Dictionary object, False returns Array object.\n",
    "        \"\"\"\n",
    "\n",
    "        c_target = self.db[target_collection]\n",
    "        # result = None\n",
    "\n",
    "        if page_size == 0:\n",
    "            # No paging, return simple find operation\n",
    "            if query_filter is None:\n",
    "                result = c_target.find(no_cursor_timeout=no_cursor_timeout)\n",
    "            else:\n",
    "                result = c_target.find(query_filter, no_cursor_timeout=no_cursor_timeout)\n",
    "        else:\n",
    "            # No query, find all\n",
    "            if query_filter is None:\n",
    "                # Not sort criteria, by default\n",
    "                if sort == '':\n",
    "                    result = c_target.find(no_cursor_timeout=no_cursor_timeout).limit(page_size).skip(\n",
    "                        page_index * page_size)\n",
    "                else:\n",
    "                    # Has sort, check sort direction\n",
    "                    if sort_descending:\n",
    "                        result = c_target.find(no_cursor_timeout=no_cursor_timeout).sort(sort, DESCENDING).limit(\n",
    "                            page_size).skip(\n",
    "                            page_index * page_size)\n",
    "                    else:\n",
    "                        result = c_target.find(no_cursor_timeout=no_cursor_timeout).sort(sort, ASCENDING).limit(\n",
    "                            page_size).skip(\n",
    "                            page_index * page_size)\n",
    "            else:\n",
    "                data = c_target.find(query_filter, no_cursor_timeout=no_cursor_timeout)\n",
    "\n",
    "                if sort == '':\n",
    "                    result = data.sort(sort).limit(page_size).skip(page_index * page_size)\n",
    "                else:\n",
    "                    if sort_descending:\n",
    "                        result = data.sort(sort, DESCENDING).limit(page_size).skip(page_index * page_size)\n",
    "                    else:\n",
    "                        result = data.sort(sort, ASCENDING).limit(page_size).skip(page_index * page_size)\n",
    "\n",
    "        if return_the_first_found:\n",
    "            if len(result) > 0:\n",
    "                return result[0]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def collection_count(self, collection):\n",
    "        return self.db[collection].count()\n",
    "\n",
    "    def update(self, collection, query, doc, upsert=False):\n",
    "        \"\"\"\n",
    "        Udate the collection document with parameters\n",
    "        :param collection: Target collection for update\n",
    "        :param query: query string\n",
    "        :param doc: doc for update\n",
    "        :param upsert: Set to True will insert new one if no match found.\n",
    "        \"\"\"\n",
    "        self.db[collection].update(spec=query, document=doc, upsert=upsert)\n",
    "\n",
    "    # TODO: Unfinished method\n",
    "    def delete(self, collection, query):\n",
    "        self.db[collection].delete_one(query)\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utilLog\n",
    "与日志相关的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility class for logging operation\n",
    "class UtilLog:\n",
    "    # Local variables\n",
    "    _db = None\n",
    "    _logger = None\n",
    "    \n",
    "    def __init__(self, disable_logging_display=True):\n",
    "        \"\"\"\n",
    "        日志工具类，负责所有日志的相关操作\n",
    "        初始化参数：\n",
    "        disable_logging_display：是否禁用输出显示而直接保存到日志库，缺省为禁用。\n",
    "        \"\"\"\n",
    "        self._db = UtilDb(db_name=DB_NAME,uri=DB_URI)\n",
    "        logging.basicConfig(level=logging.DEBUG, format=' %(asctime)s - %(levelname)s - %(message)s')\n",
    "        logger_name = 'requests'\n",
    "        if logger_name not in logging.Logger.manager.loggerDict:\n",
    "            _logger = logging.getLogger(name=logger_name)\n",
    "            _logger.setLevel(logging.critical)\n",
    "        \n",
    "        _logger=logging.getLogger(logger_name)\n",
    "\n",
    "        if disable_logging_display:\n",
    "            _logger.disable(logging.debug)\n",
    "            #logging.disable(logging.DEBUG)\n",
    "\n",
    "    def __del__(self):\n",
    "        return\n",
    "\n",
    "    # Operation log\n",
    "    def operation_log(self, log, print_to_console=False, debug_only=False):\n",
    "        \"\"\"\n",
    "        记录操作日志\n",
    "        print_to_console：记录日志时同步打印到输出窗体，缺省为否\n",
    "        debug_only：仅作调试显示而无须保存到日志数据库，缺省为否。\n",
    "        \"\"\"\n",
    "        if not debug_only:\n",
    "            log_content = {'timestamp': datetime.now(),\n",
    "                           'log': log,\n",
    "                           'log_type': 'Operation'}\n",
    "            self._db.insert(COL_LOGS, log_content)\n",
    "            _logger.debug('%s - %s' % (datetime.now(), log))\n",
    "\n",
    "            if print_to_console:\n",
    "                print('%s - %s' % (datetime.now(), log.encode('UTF-8').decode('UTF-8', 'ignore')))\n",
    "        else:\n",
    "            print('%s - %s' % (datetime.now(), log.encode('UTF-8').decode('UTF-8', 'ignore')))\n",
    "\n",
    "    # Error log\n",
    "    def error_log(self, log: '', e: Exception, print_to_console=False, debug_only=False):\n",
    "        \"\"\"\n",
    "        记录错误日志\n",
    "        print_to_console：记录日志时同步打印到输出窗体，缺省为否\n",
    "        debug_only：仅作调试显示而无须保存到日志数据库，缺省为否。\n",
    "        \"\"\"\n",
    "\n",
    "        if not debug_only:\n",
    "            log_content = {'timestamp': datetime.now(),\n",
    "                           'log': log + ' ' + str(e),\n",
    "                           'log_type': 'Error'}\n",
    "            self._db.insert(COL_LOGS, log_content)\n",
    "            _logger.error(\"Error occurred: %s\" % log + ' ' + str(e))\n",
    "            if print_to_console:\n",
    "                # print(\"%s - Error occurred: %s\" %(datetime.now(), log.encode('UTF-8').decode('UTF-8', 'ignore')))\n",
    "                print(\"%s - Error occurred: %s\" % (datetime.now(), log + ' ' + str(e)))\n",
    "        else:\n",
    "            # print(\"%s - Error occurred: %s\" %(datetime.now(), log.encode('UTF-8').decode('UTF-8', 'ignore')))\n",
    "            print(\"%s - Error occurred: %s\" % (datetime.now(), log + ' ' + str(e)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### globalvars.py\n",
    "存储全局变量\n",
    "\n",
    "```\n",
    "用于统计的全局变量\n",
    "TOTAL_FETCHED = 0  获取的记录条数\n",
    "TOTAL_SAVED = 0  保存的记录条数\n",
    "TOTAL_IGNORED = 0 忽略的记录条数\n",
    "TOTAL_ERROR = 0 错误数量\n",
    "\n",
    "用于存储数据库中的表名，以更易懂且更松散的方式来进行调用。以后即使修改了数据库表名，仅需要更新这里的相关对应即可。\n",
    "COL_ACCESS_HISTORY = 'UrlAccessHistory'  访问历史表，用于存储所有爬虫抓取的历史URL\n",
    "COL_BLACKLIST = 'UrlBlackList' 黑名单，及会被跳过的URL\n",
    "COL_ROUTINE_URL_LIST = 'RoutineUrlList' 巡回URL抓取列表，每次抓取任务都会从该表中获取并逐一执行抓取任务\n",
    "COL_ARTICLE = 'ArticleBody' 抓取文章的主体内容会保存在这里\n",
    "COL_ARTICLE_LIST = 'ArticleList' 文章列表，主要是概要信息\n",
    "COL_LOGS = 'Logs' 日志\n",
    "COL_STOCK_LIST = 'StockList' 股票列表\n",
    "COL_NLP_DICT = 'NLP_Dict' 自然语言字典\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" To store the global variables \"\"\"\n",
    "TOTAL_FETCHED = 0\n",
    "TOTAL_SAVED = 0\n",
    "TOTAL_IGNORED = 0\n",
    "TOTAL_ERROR = 0\n",
    "\n",
    "COL_ACCESS_HISTORY = 'UrlAccessHistory'\n",
    "COL_BLACKLIST = 'UrlBlackList'\n",
    "COL_ROUTINE_URL_LIST = 'RoutineUrlList'\n",
    "COL_ARTICLE = 'ArticleBody'\n",
    "COL_ARTICLE_LIST = 'ArticleList'\n",
    "COL_LOGS = 'Logs'\n",
    "COL_STOCK_LIST = 'StockList'\n",
    "COL_NLP_DICT = 'NLP_Dict'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### globalutilities.py\n",
    "可被全局调用的工具类\n",
    "\n",
    "#### UTIL_DB\n",
    "数据库操作工具类，其中的DB_URI和DB_NAME来自于configuration.py\n",
    "#### UTIL_LOG\n",
    "日志记录工具类，缺省disable_logging_display=False，如果希望不在调试输出中进行输出而仅保留数据库记录请设置disable_logging_display=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "UTIL_DB = UtilDb(db_name=DB_NAME,uri=DB_URI)\n",
    "UTIL_LOG = UtilLog(disable_logging_display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utilScraping\n",
    "与爬虫相关的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility class for web scraping operation\n",
    "class UtilScrapingOperation:\n",
    "    \"\"\" Utility class for scraping operation \"\"\"\n",
    "    _scraping_rule = None\n",
    "    _blacklist_urls = None\n",
    "\n",
    "    def __init__(self, routine_url=None):\n",
    "        \"\"\"\n",
    "        Scraping utility class to help do web scraping operations.\n",
    "        :param routine_url: the url in routine url list which will binding to specific scraping rules.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Load url black list\n",
    "        self._blacklist_urls = UTIL_DB.find(COL_BLACKLIST, query_filter={'status': 'Active'})\n",
    "\n",
    "        if routine_url is not None:\n",
    "            self.set_scraping_rule(routine_url)\n",
    "        return\n",
    "\n",
    "    def has_visited(self, url:str):\n",
    "        \"\"\"\n",
    "        Check if the specified URL has been visited before.\n",
    "        :param url: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        doc = UTIL_DB.find_one(COL_ACCESS_HISTORY,{'url':url})\n",
    "        if doc is not None and len(doc) > 0:\n",
    "            if doc['access_datetime'] != '':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def in_blacklist(self, url: str):\n",
    "        \"\"\"\n",
    "        Check if the specified URL in the blacklist\n",
    "        :param url: Partially or whole in will all return True.\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        for black_url in self._blacklist_urls:\n",
    "            if re.search(black_url['url'], url) is None:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "\n",
    "    def set_scraping_rule(self, routine_url):\n",
    "        \"\"\"\n",
    "        Set and initialize the ScrapingRule instance\n",
    "        :param routine_url:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._scraping_rule=ScrapingRule(routine_url)\n",
    "\n",
    "    def get_scraping_rule(self):\n",
    "        \"\"\"\n",
    "        Get the ScrapingRule instance\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self._scraping_rule\n",
    "\n",
    "    def fetch_urls_in_page(self, bs_obj: BeautifulSoup):\n",
    "        \"\"\"\n",
    "        获取所传入url地址页面中所有url链接并保存进数据库待抓取地址列表\n",
    "        :param bs_obj: BeautifulSoup object,页面内容\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        lst_urls = bs_obj.find_all('a', {'target': '_blank', 'href': re.compile('shtml')})\n",
    "        for url_tag in lst_urls:\n",
    "            if not self.in_blacklist(url_tag['href']):\n",
    "                history = UrlAccessHistory(url_tag['href'])\n",
    "                if not history.has_visited():\n",
    "                    history.save()\n",
    "                else:\n",
    "                    # Remove from the target url list if has been visited.\n",
    "                    lst_urls.remove(url_tag)\n",
    "            else:\n",
    "                # Remove from the target url list if found in blacklist\n",
    "                UTIL_LOG.operation_log('%s in blacklist, no save.' % url_tag['href'], debug_only=True)\n",
    "                lst_urls.remove(url_tag)\n",
    "\n",
    "        return lst_urls\n",
    "\n",
    "    def unvisited_urls(self):\n",
    "        \"\"\"\n",
    "        从UrlAccessHistory表中返回所有还未访问过的url地址\n",
    "        :return: list对象\n",
    "        \"\"\"\n",
    "        return UTIL_DB.find(target_collection=COL_ACCESS_HISTORY, query_filter={'access_datetime': ''})\n",
    "\n",
    "    def parse_and_save_article_content(self, bs_obj: BeautifulSoup):\n",
    "        \"\"\"\n",
    "        根据抓取规则抓取并保存文章内容\n",
    "        :param bs_obj: 待抓取的BeautifulSoup对象\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: Need to study how to generalize and make the reusable scraping rule model\n",
    "        scraping_rule = self.get_scraping_rule()\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_routine_urls(self):\n",
    "        \"\"\"\n",
    "        Fetch the target url list from database\n",
    "        \"\"\"\n",
    "\n",
    "        # The data structure of collection RoutineUrlList as below\n",
    "        #    {\n",
    "        #        \"_id\" : ObjectId(\"58ca0635f5e5454a930515a9\"),\n",
    "        #        \"url\" : \"http://finance.sina.com.cn/stock/\",\n",
    "        #        \"access_history\" : {},\n",
    "        #        \"scraping_rule\" : [ \n",
    "        #                                    {\n",
    "        #                                        \"field_name\" : \"title\",\n",
    "        #                                        \"tag\" : \"h1\",\n",
    "        #                                        \"property\" : \"id\",\n",
    "        #                                        \"filter\" : \"artibodyTitle\"\n",
    "        #                                    },\n",
    "        #                                    {\n",
    "        #                                        \"field_name\" : \"article_body\",\n",
    "        #                                        \"tag\" : \"div\",\n",
    "        #                                        \"property\" : \"id\",\n",
    "        #                                        \"filter\" : \"articleContent\"\n",
    "        #                                    },\n",
    "        #                                    {\n",
    "        #                                        \"field_name\" : \"article_datetime\",\n",
    "        #                                        \"tag\" : \"span\",\n",
    "        #                                        \"property\" : \"class\",\n",
    "        #                                        \"filter\" : \"time-source\"\n",
    "        #                                    },\n",
    "        #                                    {\n",
    "        #                                        \"field_name\" : \"article_editor\",\n",
    "        #                                        \"tag\" : \"p\",\n",
    "        #                                        \"property\" : \"class\",\n",
    "        #                                        \"filter\" : \"article-editor\"\n",
    "        #                                    },\n",
    "        #                                    {\n",
    "        #                                        \"field_name\" : \"article_summary\",\n",
    "        #                                        \"tag\" : \"\",\n",
    "        #                                        \"property\" : \"\",\n",
    "        #                                        \"filter\" : \"\"\n",
    "        #                                    }\n",
    "        #                                ],\n",
    "        #        \"active\" : true\n",
    "        #    }\n",
    "\n",
    "        _start = datetime.now()\n",
    "        lst_target = UTIL_DB.find(COL_ROUTINE_URL_LIST, {'active': True})\n",
    "        _duration = datetime.now() - _start\n",
    "        UTIL_LOG.operation_log('%s - 获得%s待扫描目标地址，本次操作耗时%s。' % (datetime.now(),\n",
    "                                                               lst_target.count(),\n",
    "                                                               _duration), True)\n",
    "\n",
    "        return lst_target\n",
    "\n",
    "    def fetch_url(self, url: str, force_visit=False, ignore_error=True, check_blacklist=True):\n",
    "        \"\"\" \n",
    "        Fetch web content from given url.\n",
    "        :param url: string, the target url to fetch.\n",
    "        :param force_visit: force visit even has visited before\n",
    "        :param ignore_error: Ignore raised error\n",
    "        :param check_blacklist: Check blacklist before access, bypass if exists in the blacklist.\n",
    "        :return BeautifulSoup object\n",
    "        \"\"\"\n",
    "\n",
    "        if check_blacklist:\n",
    "            if self.in_blacklist(url):\n",
    "                UTIL_LOG.operation_log('%s targeted by blacklist checking, access bypassed.' % url, debug_only=True)\n",
    "                return\n",
    "\n",
    "        if not force_visit and self.has_visited(url):\n",
    "            # Url in the blacklist or has been visited before\n",
    "            if not force_visit:\n",
    "                UTIL_LOG.operation_log('%s targeted by visited checking, access bypassed.' % url, debug_only=True)\n",
    "                return\n",
    "\n",
    "        # 清洗url格式,用\\s匹配任意的空白符，包括空格，制表符(Tab)，换行符，中文全角空格等后替换去除\n",
    "        url = re.compile(r'\\s').sub('', url)\n",
    "\n",
    "        res = requests.get(url)\n",
    "        res.encoding = res.apparent_encoding\n",
    "        UTIL_LOG.operation_log(log='%s, status code:%s' % (url, res.status_code), debug_only=True)\n",
    "\n",
    "        try:\n",
    "            res.raise_for_status()\n",
    "        except Exception as e:\n",
    "            # 发生错误,返回None\n",
    "            UTIL_LOG.error_log(e, debug_only=True)\n",
    "            if not ignore_error:\n",
    "                raise e\n",
    "            return None\n",
    "\n",
    "        bs_obj = BeautifulSoup(res.text, 'lxml')\n",
    "        return bs_obj\n",
    "\n",
    "    def fetch_article_list(self, url):\n",
    "        \"\"\" \n",
    "        Fetch article list from given url.\n",
    "\n",
    "        keyword arguments:\n",
    "        url: string, the url to fetch the article list\n",
    "        \"\"\"\n",
    "\n",
    "        _starttime = datetime.now()\n",
    "        bs_obj = self.fetch_url(url)\n",
    "\n",
    "        lst_articles = bs_obj.find_all('a', {'target': '_blank', 'href': re.compile('shtml')})\n",
    "        for i in lst_articles:\n",
    "            gap = i['href'].rfind('.shtml') - i['href'].rfind('/') - 1\n",
    "            if gap < 12 or i.string is None:\n",
    "                lst_articles.remove(i)\n",
    "\n",
    "        _executeduration = datetime.now() - _starttime\n",
    "\n",
    "        UTIL_LOG.operation_log(\n",
    "            \"%s - 从%s获得%s待处理文章项目，本次操作耗时%s\" % (datetime.now(), url, len(lst_articles), _executeduration))\n",
    "\n",
    "        return lst_articles\n",
    "\n",
    "    def parse_all_url_from_page(self, url):\n",
    "        \"\"\"\n",
    "        从所给的页面中抓取所有超链接\n",
    "        缺省规则：target=blank，以shtml结尾\n",
    "        返回：获取到的url列表\n",
    "        \"\"\"\n",
    "        _start = datetime.now()\n",
    "        bs_obj = self.fetch_url(url)\n",
    "        lst_urls = bs_obj.find_all('a', {'target': '_blank', 'href': re.compile('shtml')})\n",
    "        UTIL_LOG.operation_log('在%s找到%s个超链接。本次操作时间%s' % (url,\n",
    "                                                         lst_urls.count(),\n",
    "                                                         datetime.now() - _start))\n",
    "        return lst_urls\n",
    "\n",
    "    def fetch_next_page_url(self, url):\n",
    "        \"\"\" \n",
    "        Find the next page tag and return the url\n",
    "\n",
    "        keyword arguments:\n",
    "        url: the url page to find the tag with 下一页 displayed.\n",
    "        \n",
    "        return: string, the retrieved url or None if not found.\n",
    "        \"\"\"\n",
    "\n",
    "        _starttime = datetime.now()\n",
    "        bs_obj = self.fetch_url(url)\n",
    "        next_url_tag = bs_obj.find('a', title='下一页')\n",
    "        if next_url_tag is not None:\n",
    "            next_url = next_url_tag['href']\n",
    "            if next_url.find('./') == 0:\n",
    "                next_url1 = url.replace(url[url.rfind('/') + 1:len(url)], next_url[2: len(next_url)])\n",
    "            _executeduration = datetime.now() - _starttime\n",
    "            UTIL_LOG.operation_log(\"%s - 从%s获取下一页跳转链接成功，本次操作耗时%s\" % (datetime.now(), url, _executeduration))\n",
    "            return next_url1\n",
    "\n",
    "        else:\n",
    "            _executeduration = datetime.now() - _starttime\n",
    "            UTIL_LOG.operation_log(\"%s - 从%s获取下一页跳转链接，没有找到匹配链接。本次操作耗时%s\" % (datetime.now(), url, _executeduration))\n",
    "            return None\n",
    "\n",
    "    def parse_content_from_bsobj(self, bs_obj, target, parse_filter):\n",
    "        \"\"\" Parse content from inputted BeautifulSoup object\n",
    "\n",
    "        keyword arguments:\n",
    "        bs_obj: the BeautifulSoup object to manipulate\n",
    "        target: the html tag which are looking for\n",
    "        parse_filter: the rules to match the specific tag\n",
    "        \"\"\"\n",
    "        result = bs_obj.find(target, parse_filter)\n",
    "        if result is not None:\n",
    "            return result.get_text()\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    def update_routine_history(self, url, doc):\n",
    "        \"\"\" Update the routine history.\n",
    "        \n",
    "        keyword arguments:\n",
    "        url: the url which scraped, used to find the match from target url list.\n",
    "        doc: the document of target list which added the right history list item.\n",
    "        \"\"\"\n",
    "        UTIL_DB.update(COL_ROUTINE_URL_LIST, {'url': url}, doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utilNLP\n",
    "与自然语言相关的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UtilNLP:\n",
    "    \"\"\" Utility class for Nature Language Process \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # load user dict\n",
    "        lst_dict = UTIL_DB.find(COL_NLP_DICT, '')\n",
    "        for i in lst_dict:\n",
    "            jieba.add_word(i['word'], i['freq'], i['tag'])\n",
    "        return\n",
    "\n",
    "    def __delete__(self, instance):\n",
    "        return\n",
    "\n",
    "    def process(self, content):\n",
    "        return jieba.cut(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utilTransformation\n",
    "与数据转换相关的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UtilTransformation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def sort_document_array(self, array=None, array_to_sort='', ascend=True):\n",
    "        \"\"\"\n",
    "        Sort the input array by dict \n",
    "        :param array: the list to manipulate.\n",
    "        :param array_to_sort: the item name to sort, which must be array type.\n",
    "        :param ascend: default to True, set False to sort descend.\n",
    "        :return: the sorted array\n",
    "        \"\"\"\n",
    "        if array is None:\n",
    "            return None\n",
    "        else:\n",
    "            result = []\n",
    "            l = 0\n",
    "            for i in array:\n",
    "                l = len(i[array_to_sort])\n",
    "                offset = 0\n",
    "                flag_quit = False\n",
    "\n",
    "                if len(result) == 0:\n",
    "                    # Empty list, just append\n",
    "                    result.append(i)\n",
    "                    # print_array_order(result,array_to_sort,'append at empty')\n",
    "                else:\n",
    "\n",
    "                    for ii in result:\n",
    "                        len_of_result = len(ii[array_to_sort])\n",
    "                        # print('comparing array item(%s) > result array(%s):%s' %(l,len_of_result,l>len_of_result))\n",
    "                        if ascend:\n",
    "                            if len_of_result > l and not quit:\n",
    "                                result.insert(offset, i)\n",
    "                                flag_quit = True\n",
    "                                # print_array_order(result,array_to_sort,'insert')\n",
    "                            offset += 1\n",
    "\n",
    "                    if not flag_quit:\n",
    "                        # Still no quit means no small array found, just append\n",
    "                        result.append(i)\n",
    "                        # print_array_order(result,array_to_sort,'append at last')\n",
    "\n",
    "            return result\n",
    "\n",
    "    def print_array_order(self, array=None, array_to_sort='', action=''):\n",
    "        print('============= %s' % action)\n",
    "        for i in array:\n",
    "            print(len(i[array_to_sort]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Business logic classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 utilStock\n",
    "与股票相关的业务操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "?sys.setrecursionlimit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
